{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import argparse\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "blacklisted_words = []\n",
    "current_links = []\n",
    "indexed_words = []\n",
    "good_links = []\n",
    "warnings_level_one = []\n",
    "warnings_level_two []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_searcher(filename):\n",
    "    \n",
    "    count = 0\n",
    "    with open(filename, \"r\") as file:\n",
    "        for i in indexed_words:\n",
    "            for j in file:\n",
    "                if i in j:\n",
    "                    count += 1\n",
    "        warnings.append(\"Email contained {} blacklisted words.\".format(count))\n",
    "        if count > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_report():\n",
    "    if len(warnings_level_one) > 0:\n",
    "        print(\"We have found {} level one warnings. \\n Warnings like {}\".format(len(warnings_level_one),  str(warnings_level_one[0]))) \n",
    "    if len(warnings_level_two) > 0:\n",
    "        print(\"We have found {} level two warnings. \\n Warnings like {}\".format(len(warnings_level_two),  str(warnings_level_two[0])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Phising email analyser')\n",
    "    #Add group so either U or u is required.\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    #Arg for list of urls\n",
    "    group.add_argument('--file', '-F', metavar=\"\",help='Input location to list with urls, 1 url per line. Example -U ~/folder/list.txt')\n",
    "    #Single url. (https://google.coom)\n",
    "    parser.add_argument('--processes', '-p',metavar=\"\",type=int, help='Type how many processes you wanna run. Empty = max, which is the amount of links scraped.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    fill_good_links(\"safe_links.txt\")\n",
    "    index_file(args.file)\n",
    "    scan_links(current_links)\n",
    "    word_searcher(args.file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_good_links(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        good_links.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_links(list_of_links):\n",
    "\n",
    "    # Printing the list of links to check for formarting\n",
    "    print(list_of_links)\n",
    "    print(index_words)\n",
    "    print(good_links)\n",
    "    print(warnings)\n",
    "    state = False\n",
    "    for i in list_of_links:\n",
    "        if \"http\" in i or \"https\" in i:\n",
    "            for j in good_links:\n",
    "                if j in i:\n",
    "                    state = True\n",
    "                else: continue\n",
    "\n",
    "            if state is True:\n",
    "                break\n",
    "            elif state is False:\n",
    "                warnings.append(\"Unkown Links in Email.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_file(email):\n",
    "    \n",
    "    # Isolating format.\n",
    "    file_format = email.split(\".\") \n",
    "    print(file_format)\n",
    "\n",
    "    if file_format[1] == \"txt\":\n",
    "        # Indexing index_words\n",
    "        with open(email, \"r\") as f:\n",
    "            for line in f:\n",
    "                indexed_words.extend(line.split())\n",
    "\n",
    "        for i in index_words:\n",
    "            if \"http\" in i or \"https\" in i:\n",
    "                current_links.append(i)\n",
    "\n",
    "\n",
    "    elif file_format[1] == \"html\":\n",
    "        print(\"Check html code\")\n",
    "        get_links(email)\n",
    "    else:\n",
    "        # Indexing index_words\n",
    "        with open(email, \"r\") as f:\n",
    "            for line in f:\n",
    "                indexed_words.extend(line.split())\n",
    "        \n",
    "        for i in index_words:\n",
    "            if \"http\" in i or \"https\" in i:\n",
    "                current_links.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(filename):\n",
    "    try:\n",
    "        with open('filname', 'rw') as source:\n",
    "            soup = bs.BeautifulSoup(source, 'lxml')\n",
    "        #Find all the href links in the source code.\n",
    "            links = [link.get('href') for link in soup.find_all('a')]\n",
    "        #Remove outgoing links, only keep local links.\n",
    "            for link in links:\n",
    "                if filename in link:\n",
    "                    #if filename is in link, add to list.\n",
    "                    current_links.append(link)\n",
    "                elif not 'http' in link:\n",
    "                    #if name has a . or / add to list\n",
    "                    if '/' in link[0] or '.' in link[0]:\n",
    "                        new_link = ''.join([filename,link])\n",
    "                        current_links.append(new_link)\n",
    "                    #if not, add / to item and then add to list.\n",
    "                    else:\n",
    "                        new_link = ''.join([filename,'/',link])\n",
    "                        current_links.append(new_link)\n",
    "                else:\n",
    "                    pass\n",
    "    #check if errors.\n",
    "    except TypeError as e:\n",
    "        print(e)\n",
    "        print(\"Got a Typerror\")\n",
    "        return []\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    #Instead of dispalying the error, put into error log.\n",
    "    except Exception as e:\n",
    "        with open('error.txt', 'w') as f:\n",
    "            f.write(str(e))\n",
    "        return[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hello 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
